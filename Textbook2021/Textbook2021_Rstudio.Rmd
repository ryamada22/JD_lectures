---
title: "ゲノム・オミックスのデータ解析教室(R版)"
author: "ryamada"
date: "2021年8月21日"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## データ解析上の特徴

### はじめに

ゲノム・オミックス解析に用いられるデータは、ハイスループット実験による標本のゲノム・エピゲノム・トランスクリプトーム等のオミックスデータと、標本に関するデータに分けて整理すると理解しやすい。

標本に関するデータの例を挙げる。
標本が個体由来であれば、個体の特徴を表すデータであり、年齢・性別・疾病の有無・臨床検査値などがこれに相当する。

標本がシングルセルであれば、シングルセルについて実験とは別に得られるデータが相当し、シングルセルの由来元である個体の情報や、シングルセルの採取時期・場所等に関するデータ、シングルセルの形態等のデータが含まれる。

なぜ、ゲノム・オミックス実験のデータとそれ以外のデータを分けるとわかりやすいと言うのかの理由は次の通りである。

ゲノム・オミックス以外のデータは、いわゆるデータサイエンス・統計解析における多変量解析の変数のデータとして得られ、データ解析に用いる際に、通常の解析フローにすぐに乗せることができる。
それに対して、ゲノム・オミックス実験のデータは、データの取得に際して多数の変数に関して一塊として得られるという特徴があり、データ解析に用いる前に、データ全体の評価を行い、その全体の記述統計結果に基づいて、調整するステップが必要となる点で、それ以外のデータと異なるからである。

また、データ解析は大きな括りで言えば、多変量解析・多変量データマイニングであると言えるが、ほぼすべてのデータ解析において、一次出力を行った後、その結果の適正な解釈のために、さらなる処理を要するという特徴がある。
この点が、ゲノム・オミックス研究のデータ解析に比較的、特有である。

以下に、ゲノム・オミックスのデータ解析に特徴的な点を順次説明する。

### 解析されるデータ(入力データ)の特徴

#### ゲノム・オミックス実験データの特徴

ハイスループット実験という名称がその特徴をよく表している。 一度に、１標本について、非常にたくさんの変数のデータが一塊として得られるのがハイスループット実験であり、ゲノム・オミックス実験データの特徴である。

例えば、数十万超の一塩基多型のジェノタイプや万の単位の遺伝子の発現量を、一標本について一度に得るチップ実験がそれである。 また、パラレル・シークエンシング技術によりゲノム・エピゲノム・トランスクリプトームに関する定量データを、一標本について一度に得る実験もその例である。

このハイスループット実験から得られる情報の特徴は、個々のデータに質の良し悪しのばらつきがあること、一塊で得られるデータ全体からデータの質の良し悪しの全体としての傾向の情報が得られること、標本ごとに一塊で得られるデータセットを標本間で比較可能にするためには、個々の標本のデータセットの全体の傾向を標本間で比較し、その比較結果に基づいて調整するステップが必要になることである。

この解析前の処理は、標本ごとのデータセットの質的記述統計と、質的記述統計に基づくデータの採否決定と値の標準化とを含む。

また、ゲノム・オミックス実験データは、その一次出力データをそのままデータ解析に使えないことが多いことも特徴的である。 パラレル・シークエンシングの一次出力データは多数のショートリードであるが、ショートリードの情報そのものはゲノム・オミックス的な情報としては不十分である。 レファレンス配列との比較照合(ショートリードのマッピング・アラインメント)を経て、ショートリードの分子遺伝学的意味付けを行う必要がある。 さらに、ショートリードがレファレンス配列上に付与した情報を基に、ゲノム配列の状態(多型性)やゲノム・クロマチンの化学修飾状態・アクセスしやすさ、遺伝子発現量・トランスクリプトバリアント別発現量などの、データ解析するための変数のデータに変換される。

このデータ解析用の変数データへの変換に当たって、統計学的に推定処理が施される。

このようにデータ解析の前段階において、記述統計、アラインメント処理、推定などが行われるわけだが、本テキストでは、これらの処理について、データ解析の前段階の一部として説明することはしない。

これらの処理は、統計学的処理・機械学習であり、いわゆるデータの本解析の考え方が同一であるので、それらと共に扱うこととする。 そのような扱いをすることで、統計学的処理・機械学習の立場で考慮するべき基本的な考え方を押さえれば、前処理も本解析も解析結果の解釈も行えることを強調したい。

#### その他のデータの特徴

標本の属性は、個体の属性にしろシングルセルの属性にしろ、標本ごと・特徴量ごとにそれぞれ観測する場合が多く、データ値そのものが多変量解析にそのまま使えることが通例である。従って、一塊での観測に伴う記述統計や標本間での標準化は不要であることが普通である。

また、一次観測データの変換も不要であることが多い。

もし、観測が一斉に行われたり、観測が複合的なものであって、そこから何らかの特徴量を抽出する必要がある場合などは、前項で説明したのと同様の考え方を適用すればよい。 そのような例としては、画像情報からの形態特徴量抽出などが含まれるかもしれない。

### 解析スタイルの特徴

データの本解析は、入力データを統計学的・機械学習的に評価し、メッセージを引き出すことである。 その意味では、ゲノム・オミックス解析が特殊な手法を用いているわけではない。

ゲノム・オミックスのデータ解析では、評価対象になる変数の数が膨大であり、その膨大な変数の組合せや相互作用を積極的に考慮する姿勢が強いことが特徴となっている。

変数の組合せ・相互作用を取り扱うのは、オミックス(-ome)という概念自身が、多数の要素の総体としての働きの理解を目指すことと定義されているから当然である。

また、多数の要素(多数の遺伝子多型、多数の遺伝子の発現量など)を同時に取り扱うことから、解析の過程で、その多数の要素が作る分布に関する情報を得ながら、その情報を解釈に活用していくことも多い。
これは、データ駆動型にデータ全体からメッセージを読み取るという特徴であると言い換えることができる。
多数の要素をいちどきに評価することで得られる情報は、変数の観測値の分布だけでなく、各要素ごとに得られる統計量や推定量の分布も含まれる。

以下では、多変量解析と多段階解析とデータ駆動型解析いう３つの視点から、ゲノム・オミックスデータ解析の特徴を概観する。

#### 多変量解析と多段階解析

ゲノム・オミックスデータ解析は、多数の変数を用いるという点では多変量解析と言える。 しかしながら、要素の一次線形和での説明を目標としたり、変数選択によって寄与変数の数を減らすことで良しとするという形式の多変量解析ではない。

より複雑で巧妙な仕組みを理解することを目標に、説明変数の組合せをさらに階層的に組み合わせたり、非線形性を用いた解釈を目指す解析となっている。 この点で、従来型の多変量解析とは異なる。

説明変数の組合せ解析は、要素の組合せ爆発の問題があるために、統計・機械学習的な組合せ探索にはおのずと限界がある。 ゲノム・オミックス解析では、要素の組合せを考慮するにあたり、生物学的な事前知識を投入することが多い。 このような生物学的な事前知識としては、オントロジー情報やパスウェイ情報が良く用いられる。

また、同一遺伝子・同一分子に紐づく複数の要素を組合せとして用いることもある。

このような生物学的な情報を用いて複数の要素の組合せを評価する場合、個々の要素についてデータ解析上の出力をいったん行い、その一次出力を要素の組合せとして統合するという手続きを踏むアプローチが取られることが多い。 また、組合せ評価をしたうえで、複数の組合せ同士の関係性の評価をすることで、階層的な組合せの評価を行うことができる。

このようなアプローチでは、多段階の処理が施されていることがわかる。

非線形性を持ち込んだ解釈をすることは、現象自体に非線形性があるので重要である。 しかしながら、非線形モデルを設定し、最適なモデルパラメタの推定をするというアプローチは得策ではないことが多い。 なぜなら、ゲノム・オミックスの解析では変数の数が膨大であるため、線形モデルであっても自由度・次元が十二分に高く、それが解析の実行上のハードルになっており、そこに、変数の寄与として非線形性を取り込むと、解析の自由度・次元がさらに高くなり、非現実的であるからである。

そのような直接的な非線形モデルの活用ではなく、手法の中にうまく非線形処理を取り入れるアプローチが取られることが多い。 詳細は後述するが、例えばニューラルネットワークを用いたり、サポートベクタマシンを用いれば自然と非線形性を持ったデータマイニングになることが、その例として挙げられる。 また、ノンパラメトリックなモデルフィッティングを行うことでデータ駆動型のモデル推定が可能となるが、これも、そのような例と考えられる。

他の例としては、次元削減の手法の中に非線形次元削減法として分類されるものがあるが、これも非線形な処理結果をもたらす手法の例である。

### データ駆動型解析と多段階解析

ゲノム・オミックス研究では、多数の要素について一度にデータが得られるので、その要素の分布についての情報が得られる。

その分布に関する情報は既知の場合もあるかもしれないが、実験をして初めて得られる情報であることも多い。

そのような新たに得られた情報を利用することで、個々の統計的・機械学習的出力を再評価することが適切であることもある。
データを集めてみてから、そのデータの解析・解釈をどのようにするかの方針が決まる部分があると言うことである。
このようなアプローチはデータ駆動型なアプローチと言える。

このアプローチでは実質的に２段階の解析(多段階解析)が行われていることがわかる。

１段階目で得られる結果が、多数の帰無仮説検定結果のセットであるときに、多数の検定を行ったことに留意して、棄却の判定を調整する場合には、２段階目は多重検定補正と呼ばれる処置に相当する。

１段階目の結果を用いて２段階目を行い、２段階目の結果の解釈に当たって１段階目の結果が得られたということの影響を加味することとすれば、Selective inferenceに相当する。

この１段階目と２段階目とをそれぞれベイズ流に行えば、階層型ベイズと呼ばれるアプローチに相当する。

２段階処理の形を取らないものの、多数の要素の分布の情報を用いて、個々の要素に関する推定の個別最適化ではなく、全要素の推定の包括的最適化を行うことで、個々の要素の推定結果が変わるような方法もある。
これも、２段階アプローチと考え方を共有しているとみなせるだろう。

### 例

以下の例では、いくつかの具体的な解析手法が登場するが、その詳細についてこだわることなく、ゲノム・オミックスデータ解析の特徴との関係に主眼を置いて、理解することが望ましい。

#### 多変量の組合せ爆発

$n$個の要素の効果を調べるとき、個々の効果を調べれば、$n$通りの評価をすることになる。
組合せを考える場合には、ペアであれば$\frac{n(n-1)}{2} = \begin{pmatrix}n\\2 \end{pmatrix}$通り。
トリオを考えれば、$\frac{n(n-1)(n-2)}{3\tines 2 \times 1} = \begin{pmatrxi}n\\ 3 \end{pmatrix}。

$n$個の要素のすべてが集まって生まれる効果について考えると、$\frac{n!}{0!n!} = \begin{pmatrix}n \\ n \end{pmatrix} = 1$。

これらのすべての場合を足し合わせると、
$$
2^n -1 = (1+1)^n -1
$$

組合せは$2^n$という大きな値になる。
特に$n$が少しばかり大きくなると、手に負えない組み合わせ数になる。
これが組合せ爆発である。

具体的には以下のようになる。

$$
1 = 1 = 2^1 -1\\
2 + 1 = 3 = 2^2 -1\\
3 + 3 +1 = 7 = 2^3 -1\\
...\\
\begin{pmatrix}n\\1 \end{pmatrix}+\begin{pmatrix}n\\2 \end{pmatrix}+ \cdots + \begin{pmatrix}n \\ n \end{pmatrix} = 2^n -1
$$

このような組合せを、いわゆる多変量回帰で行うとき、次のようにする。

100標本、4説明変数のデータを作ってやってみる。
```{r}
n.sample <- 100
n.feature <- 4
X <- matrix(rnorm(n.sample*n.feature),ncol=n.feature)
# Y はXを用いて適当に作った値とする
Y <- apply(sin(X),1,sum)

my.data <- data.frame(Y,X)
```

単純な線形回帰では、長さが説明変数のベクトル$\mathbf(a) = (a_1,...,a_{n.feature})$を使って、以下のように表される。


$$
y_i \sim b + \sum_{j=1}^{n.feature} x_{i,j} \times a_j 
$$

この回帰は以下のように実施する。
切片項の値(Intercept)と、４つの説明変数の係数が推定される。

```{r}
lm.out <- lm(Y~X1+X2+X3+X4,data=my.data)
lm.out$coefficients
```

この方式で、説明変数のペアも考慮するとすれば、以下のようにする。こうすることで、ペア項($X_1 \times X_j$)の係数も推定される。

```{r}
lm.out2 <- lm(Y~(X1+X2+X3+X4)^2,data=my.data)
lm.out2$coefficients
```

$n$個の組合せまですべて評価することにするなら、以下のようにすることができる。

```{r}
lm.out4 <- lm(Y~(X1+X2+X3+X4)^4,data=my.data)
lm.out4$coefficients
```

以上のように、組合せを考慮すると、非常に多くの項が現れ、推定する係数の数も非常に多くなる。

しかしながら、線形回帰法を適用する限りは、項の数が多くなっても、同じ枠組みで解析の処理が行われていることに注意しておこう。

線形回帰は線形代数計算により係数の推定値が決定論的に求められるが、線形代数計算は、計算機が最も得意とする処理であり、その意味では、この例では、項数の増大のみが解析上の問題になるっている。

#### パスウェイ解析とGene-set enrichment analysis(GSEA)の考え方

トランスクリプトーム解析により、すべての遺伝子の発現量を観測し、その発現量が２群間(疾病群と健常群など)で異なるかどうかを、個々の遺伝子について比較する解析があります。

この比較のやり方には幾つかの方法がありますが、例えば、２群間の発現量を平均値の差の検定により、平均値に差がないという帰無仮説が棄却されるか否かでラベル付けをする、という方法があります。

全遺伝子の数が$N$、そのうち、帰無仮説が棄却された遺伝子の数が$M$とします。

ここで、$N$個の遺伝子のうち、$K$個の遺伝子のセットを取り出すことにします。

この$K$個の遺伝子は、ある『パスウェイ』に乗っている遺伝子であるとしましょう。
この『パスウェイ』とは、過去の色々な研究の結果、遺伝子同士・遺伝子がコードする分子同士に相互作用があることが判明した結果、データベースなどに登録されているものです。

この$K$個の遺伝子は、帰無仮説を棄却したものと棄却しないものとに分けられるので、その個数を$k_m$個と$K-k_m$個と書くことにします。

そうすると以下のような$2 \times 2$分割表が得られます。


|                | Rejected | Not-Rejected      | 
|:---------------: | :--------: | :-----------------: | 
|In Pathway      | $k_m$      | $K - k_m$           | 
|Not-In Pathway  | $M - k_m$  | $(N-M) - (K - k_m)$ | 

このパスウェイ上の遺伝子が、棄却遺伝子群にエンリッチしている(非棄却群に比べて棄却群に偏っている)かどうかは、この分割表の独立性の検定によって評価することが可能です。

以下は、その例を示しています。

フィッシャーの正確確率検定によるp値が$10^{-6}$オーダーと小さいので、このパスウェイの遺伝子は棄却群にエンリッチしていると言えそうです。
このことは、このパスウェイの働きと２群間の発現量の違いとに関連があることを示唆していると考えても良さそうです。

```{r}
N <- 25000
M <- 1000
K <- 60
km <- 12

tab <- matrix(c(km, K - km, M - km, (N-M)-(K-km)),byrow=TRUE,2,2)
print(tab)
```

```{r}
f.test.out <- fisher.test(tab)
print(f.test.out)
```

この例では、多数の要素を個別に評価し、その結果を用いて、要素の集合について、さらなる評価を行うという２段階処理をしている。

また、要素の集合は生物学的な知識を活用して作成している点も、ゲノム・オミックスデータ解析のバイオインフォマティクス的特徴を表している。

#### 非線形次元削減の例 UMAP

ゲノム・オミックスデータは非常に高次元である。トランスクリプトームで言えば、遺伝子の数が次元になるので、二万超次元の空間に、標本がたなびく雲のように広がっている。

シングルセル・トランスクリプトーム解析では、多数のシングルセルの高次元の広がりの様子を２次元に次元縮減し、互いに似通った発現パターンの細胞のクラスタを存在と、クラスタの相互の遠近関係を視覚化することがある。

高次元空間の広がりを低次元空間に押し込むためには、高次元空間で曲がって広がっている細胞を平たくして低次元空間に押し込めることになるので、「非線形」な処理となる。

このことから「非線形次元削減」と呼ぶ。

その一手法であるUMAPの例を示す。

200細胞、90遺伝子のシングルセル発現量データを用いて、UMAP法による2次元非線形埋め込みを実施している。

UMAP法の出力が２次元座標であることが出力から見て取れる。

その座標を用いて、200個の細胞を２次元平面に埋め込んだ様子を視覚化して見せる。

特定の遺伝子の発現量の多寡で色を変えてプロットすることで、細胞のクラスタリングの具合と、その遺伝子の発現量との関係も視覚的に示すことができる。

また、UMAP法では実施条件のパラメタがあり、その値ごとに出力が変わる様子も示している。

```{r}
# install.packages("umap")
# install.packages("iCellR")
library(umap)
library(iCellR) # single cell data set "demo.obj@raw.data"のため
data <- demo.obj@raw.data
custom.settings = umap.defaults
custom.settings$n_neighbors = 3

umap.out <- umap(data,config=custom.settings)
head(umap.out$layout)

```
```{r}
plot(umap.out$layout,pch=20)

# 特定の遺伝子発現量の多寡で色塗りしてみる
# 色塗りの具合の見栄えのために、無意味な値変換をしているので、その点は無視してほしい
col <- data[,12]
col <- (col-min(col))/(max(col)-min(col)) 
col <- col^0.2
plot(umap.out$layout,pch=20,col=rgb(col,1-col,rep(1,length(col))))
```

UMAP実行条件パラメタを変えてみる。

２次元プロットの様子は随分、変化することが見て取れます。

```{r}
custom.settings = umap.defaults
custom.settings$n_neighbors = 10

umap.out2 <- umap(data,config=custom.settings)

plot(umap.out2$layout,pch=20)

# 特定の遺伝子発現量の多寡で色塗りしてみる
# 色塗りの具合の見栄えのために、無意味な値変換をしているので、その点は無視してほしい
col <- data[,12]
col <- (col-min(col))/(max(col)-min(col)) 
col <- col^0.2
plot(umap.out2$layout,pch=20,col=rgb(col,1-col,rep(1,length(col))))
```

この例は非線形性の手法を説明している。
非線形性の手法は、柔軟な評価ができることを特徴の１つとする。
柔軟な評価を可能にする手法の多くは、実行条件をパラメタで指定する方式を取る。
それには次のような理由が挙げられる。
極めて柔軟な手法では、出力を選ぶ自由度が高くなっているが、出力を選ぶ範囲を広げ過ぎると、出力を選び出すアルゴリズの計算量が多くなりすぎるという問題が生じる。
その問題を緩和するために、実行条件をパラメタで指定することで、パラメタ依存的に、出力の範囲を絞るという方法が取れる。
このようにして、出力の自由度を大きくしつつ、実際の出力決定に当たっては、ある程度の制約を施すことで、処理として現実的なものとしている。
また、出力がパラメタ依存になることから、線形手法の決定論的特性に比べて、非決定論的側面も有することがわかる。

#### マルチプルテスティング補正の考え方 FWER

帰無仮説検定で算出されるp値は、「帰無仮説が真」であるとの仮定に立った時、得られたデータと同じかそれよりも帰無仮説から逸脱したデータが得られる確率として定義される。

その定義を満足するように検定手法は設計され、その結果、帰無仮説が真であるとき、p値は0から1の一様分布を取る。

今、N個の独立した検定を行うことを考える。
N個すべての検定で帰無仮説が真であるとする。
それぞれの検定のp値は一様分布に従う。

独立な確率事象が同時に起きる確率は、個々の事象の確率の積となるから、N個の検定が相互に独立ならば、すべてのp値がある値 $\alpha$未満である確率は、$\alpha^N$となる。

逆に、すべてのp値が$\alpha$より大きい確率は、$(1-\alpha)^N$となる。

従って、N個の検定の全てで帰無仮説が真なのに、「たまたま」棄却してしまう検定が１個もない確率は、検定閾値を$\alpha$としたとき、
$$
(1-\alpha)^N
$$
となる。

逆に言うと、１個以上の仮説を棄却してしまう確率(１個以上の偽陽性を出してしまう確率)は
$$
1-(1-\alpha)^N
$$
となる。

従って、偽陽性を１個以上、出してしまう確率を、$t$(例えば、0.05)にしたければ、

$$
1 - (1-\alpha)^N = t=0.05
$$
を満足するような$\alpha$を閾値にして、個々の検定のp値の棄却の判断をすればよい。

そのような棄却水準は、以下の式で与えられる。

このようにして設定する棄却水準で個々の検定のp値(素のp値、nominal p-値)の棄却を再評価する方法を、Family-Wise error rate法による多重検定補正法と言う。

$$
\alpha = 1-(1-t)^{\frac{1}{N}} 
$$

今、
$$
f(\alpha) = (1-\alpha)^N = 1-t
$$
を$\alpha=0$で級数展開し、その第２項までで近似すると

$$
f(\alpha) = \sum_{i=0}^{\infty} \frac{1}{i!} f^{(i)}(0)\alpha^i\\
\hat{f}(\alpha) = f^{(0)}(0)\alpha^0 + f^{(1)}(0)\alpha^1 \\
 = 1 - N \alpha = 1- t
$$

従って
$$
\alpha \sim \frac{t}{N}
$$

これは、素のp値が棄却水準 t (=0.05)を検定数$N$で割って小さくした値より小さいときに初めて、帰無仮説を棄却するということになる。

この基準はBonferroni法と呼ばれる方法に相当している。

Bonferroni法は単純で保守的(偽陽性が出にくくなる)方法なので、頻用される多重検定補正法である。
$$
f^{(1)}(t) = \frac{d}{dt}f(t) = \frac{1}{N}(1-t)^{\frac{1}{N}-1}\\
f^{(2)}(t) = \frac{d}{dt}f^{(1)}(t) = (-1) \frac{1}{N}(\frac{1}{N}-1)(1-t)^{\frac{1}{N}-2}\\
...\\
f^{(k)}(t) = \frac{d}{dt}f^{(k-1)}(t) = (-1)^{k-1} \prod_{i=1}^k (\frac{1}{N}-(i-1)) \times (1-t)^{\frac{1}{N}-k}
$$

$$
\alpha = \sum_{i=0}^{\infty} \frac{1}{i!} f^{(i)}(t=0)
$$

$N$の値を変えて、$1-(1-\alpha)^{\frac{1}{N}}$の値と$\frac{\alpha}{N}$の値を比較してみる。

タイプ１エラーを0.05で比べると、両法にほとんど差がないことが判る。
```{r}
Ns <- 1:10000
alpha <- 0.05
t.bonferroni <- alpha/Ns
t.FWER <- 1-(1-alpha)^(1/Ns)
matplot(log10(Ns),cbind(t.bonferroni,t.FWER),type="l",ylab="閾値",main="多重検定閾値と検定数の関係,BonferroniとFWER")
plot(t.bonferroni,t.FWER,xlab="Bonferroni閾値",ylab="FWER閾値",main="Bonferroni閾値とFWER閾値の違い",type="l")
abline(0,1,col=2)
```

両法の違いを際立たせるために、タイプ１エラーの値を大きくして視覚化すると以下のようになる。

黒のライン(Bonferroni法)の閾値の方が赤のライン(FWER法)より小さめになることがかろうじて見て取れる。

両法の差がわずかだが、Nが大きくなると、１個、数個の偽陽性の有無への影響は増幅されることに注意する。


```{r}
Ns <- 1:10000
alpha <- 0.2
t.bonferroni <- alpha/Ns
t.FWER <- 1-(1-alpha)^(1/Ns)
matplot(log10(Ns),cbind(t.bonferroni,t.FWER),type="l",ylab="閾値",main="多重検定閾値と検定数の関係,BonferroniとFWER")
plot(t.bonferroni,t.FWER,xlab="Bonferroni閾値",ylab="FWER閾値",main="Bonferroni閾値とFWER閾値の違い",type="l")
abline(0,1,col=2)
```

多重検定補正においては、１段階目で個々の検定の素のp値を出し、２段階目で、そのp値の解釈を多重検定の文脈で調整していると言う意味で、２段階処理になっている。

FWER、Bonferroni法においては、いくつの検定を行ったかという情報のみを用いて２段階目の処理が行われている。

#### マルチプルテスティング補正の考え方2 FDR

FWER法とBonferroni法では、すべての帰無仮説が真であることを想定してタイプ１エラーをコントロールしていました。

FDR(False Discovery Rate)法では、N個の仮説のうち、ある割合$\beta$では帰無仮説が偽で、残りの仮説で帰無仮説が真だと言う前提で考えます。

このとき、N個の検定のp値を小さい順に並べ、$i$番目に小さいp値に対して、$\beta \times \frac{i}{N}$という値を棄却水準にすることにします。

この棄却水準を下回る小さなp値のうち、最も大きいp値が$k$番目に小さいp値だったとき、小さい方から$k$番目までのp値を持つ仮説の帰無仮説を棄却します。

このようにすることで、帰無仮説が棄却される仮説のうち、割合がおよそ$\beta$の割合の仮説が偽陽性になります。

この偽陽性の割合のことを、FalseにDiscoveryされたRateと称して、FDR法と称します。

ここで書いた、p値の大小順別の棄却水準の決め方はFDR法の１つで、Benjamini＆Hochberg法(BH法)と呼ばれれます。

わかりやすく、よく使われる方法です。

以下のように考えます。

仮説のうち$\beta$の割合の仮説は、対立仮説が真であるので、標本数が無限大だと、p値は0とみなせます。残りの$1-\beta$の割合の仮説のp値は一様分布を取ります。

従って、p値を小さい順に並べると、横軸xの値が0から$\beta$までは、縦軸$y=0$、xが$\beta$から1までは、$y = \frac{1}{1-\beta}(x-\beta)$の直線になります。

他方、検定棄却水準は、横軸xに応じて変わりますが、その値は$y_{thres} = \beta x$という直線です。

この交点を計算すると
$$
(x,y) = (\frac{\beta}{\beta^2-\beta+1},\frac{\beta^2}{\beta^2-\beta+1})
$$
となる。

検定棄却直線よりも昇順p値の折線が下に来る割合は、$\frac{\beta}{\beta^2-\beta+1}$、そのうち、対立仮説が真なのは$\beta$、帰無仮説が真なのは$\frac{\beta}{\beta^2-\beta+1} - \beta$になる。

棄却される仮説に占める、帰無仮説が真である仮説の割合(FDR)は

$$
FDR= \frac{\frac{\beta}{\beta^2-\beta+1} - \beta}{\frac{\beta}{\beta^2-\beta+1}} = \beta - \beta^2
$$

$\beta$の値が小さめだとすると$\beta^2$は無視できるくらい小さいとすれば、

$$
FDR \sim \beta
$$
となり、棄却仮説に占める偽陽性の割合が$\beta$くらいにコントロールできることになる。

昇順p値の折線と棄却水準の線とを描く。

黒線は $y=x$の線、青線は、理論的・理想的なp値の累積分布に相当する折線。
赤線は、p値の昇順ごとに異なる帰無仮説棄却閾値を示す。

```{r}
t <- seq(from=0,to=1,length=1000)
beta <- 0.2
thres.x <- t
thres.y <- 1 * t
plot(thres.x,thres.y,type="l",main="p値累積カーブとFDR閾値",xlab="sorted p-values",ylab="quantile",asp=TRUE)
abline(v=c(0,1))
abline(h=c(0,1))
abline(0,beta,col=2)
segments(0,0,beta,0,col=4)
segments(beta,0,1,1,col=4)
X <- beta/(beta^2-beta+1)
Y <- beta^2/(beta^2-beta+1)
points(X,Y,pch=20,cex=2)
```

青線と赤線との交点付近を拡大した図を描く。

```{r}
t <- seq(from=0,to=1,length=1000)
beta <- 0.2
thres.x <- t
thres.y <- 1 * t
plot(thres.x,thres.y,type="l",main="p値累積カーブとFDR閾値(拡大図)",xlab="sorted p-values",ylab="quantile",xlim=c(0,beta*2),ylim=c(0,beta^2*2))
abline(v=c(0,1))
abline(h=c(0,1))
abline(0,beta,col=2)
segments(0,0,beta,0,col=4)
segments(beta,0,1,1,col=4)
X <- beta/(beta^2-beta+1)
Y <- beta^2/(beta^2-beta+1)
points(X,Y,pch=20,cex=2)
points(X,0,pch=20,cex=2)
points(beta,0,pch=20,cex=2)
abline(v=beta)
```


次に、$\beta$の値と、FDR($\beta-\beta^2$)の値との関係を描く。

赤の直線がBH法で目指しているFDRの値。
黒の曲線が、BH法で実現されると予想されるFDRの値。

$\beta$値が小さめのときには、両者に大差がないことが判る。

```{r}
betas <- seq(from=0,to=0.6,length=100)
fdrs <- betas - betas^2

plot(betas,fdrs,type="l",main="BH法がFDRを近似する様子",xlab="parameter beta",ylab="FDR")
points(betas,betas,type="l",col=2)

```
多重検定補正においては、１段階目で個々の検定の素のp値を出し、２段階目で、そのp値の解釈を多重検定の文脈で調整していると言う意味で、２段階処理になっている。

FDR法においては、いくつの検定を行ったかという情報だけではなく、個々のp値が全p値の中で何番目に小さいかの情報も使って２段階目の処理を行っている。

#### Selective inference の考え方

多段階解析の例としてSelective inferenceを取り上げる。

Selective inferenceを個々の問題に適用して、適切な判断をするための手法は発展途上の部分が多いのが現状である。

従って、ここでは、その考え方の枠組みを紹介することとする。

そして、この問題への対処が、医学・生命科学の研究成果報告の再現性の乏しさの改善に資すると期待されていることを理解することを、本書での目標とする。

以下に例を挙げる。

きれいな正規分布に従う２個の変数があり、多数の標本についてその変数の値を観測したとする。


今、２群にクラスタリングする。これを第一ステップとする。

そのようにして出来た２群に対して、平均値の差の検定をしてみる。

```{r}
n.sample <- 1000
n.feature <- 2
X <- matrix(rnorm(n.sample * n.feature),ncol=n.feature)
hist(X[,1])
hist(X[,2])
plot(X,pch=20,xlab="X1",ylab="X2",asp=TRUE)
```
kmeans法で２群に非階層クラスタリングする。

２群を色分けして表示する。

```{r}
km <- kmeans(X,2)
plot(X,asp=TRUE,pch=20,col=km$cluster,xlab="X1",ylab="X2",main = "Result of kmeans clustering (No. clusters = 2)")

```

第一段階で、標本が２群に分かれたので、その情報を使って、第一の説明変数について、２群間で平均値の差があるかどうかをt-検定してみる。
このt-検定が２段階目の処理である。


```{r}
my.data <- data.frame(cl=km$cluster,X)
head(my.data)
t.test(X1 ~ cl, data=my.data)
```

２群間の平均値の差がないという帰無仮説は非常に小さなp値で棄却される。

この結果の解釈について考えよう。

そもそも、標本の２変数の分布はキレイな二次元正規分布であり、２群に分ける根拠は全く無かった。

しかしながら、kmeans法を適用することによって、無理やり２群に分けてやった。
分けるに際して、２変数を使って分けたので、その２群の間で２変数の値に違いがあるのは、当然のことであり、２群間でその変数について差があるかどうかを検定したときに、統計的有意差を持って差があると判定されることは驚くに値しない。

p値は「そんなに違うとは驚きだ」という気持ちを数値にしたものなので、「驚くに値しない」のであれば、このp値は「驚き」の程度を適切に数値化しているとは考えがたい。

Selective inferenceの基本的な考え方は、このようなものである。

この例では、クラスタリングをしたことがそもそも不適切であったので、この結果の解釈のおかしさを、そのせいだと考えるかもしれない。

次の例をみてみよう。

明らかに母集団が２群に分かれているとする。

但し、母集団からサンプルすると、その２群の標本の取る値にはある程度の重なりがあるものとする。

以下に図で示す。

```{r}
n.sampleA <- 100
n.sampleB <- 100
n.feature <- 2
XA <- matrix(rnorm(n.sampleA * n.feature,0,1),ncol=n.feature)
XB <- matrix(rnorm(n.sampleB * n.feature,0,1),ncol=n.feature)
XB[,1] <- XB[,1] + 1

AB.label <- c(rep(1,n.sampleA),rep(2,n.sampleB))
X.AB <- rbind(XA,XB)

plot(X.AB,asp=TRUE,pch=20,col=AB.label,xlab="X1",ylab="X2",main="２群の分布")

```

標本が２群のどちらの由来かが判ったものとして、上図は描いたが、由来群が不明であったとする。

群をkmeans法で推定してみる。

```{r}
km2 <- kmeans(X.AB,2)
plot(X.AB,asp=TRUE,pch=20,col=km2$cluster,xlab="X1",ylab="X2",main = "Result of kmeans clustering (No. clusters = 2)")

```

色分けの具合を見ると、スパっと２群に分かれている。
このkmeans法による２群分けは、真の群とは異なっていることも見て取れる。

では、２群の帰属情報が、真の情報の場合と、kmeans法で推定した場合とで、２群間の平均値の差の検定をしてみる。

どちらも、非常に小さいp値を返してるが、kmeans法のクラスタリングの結果を利用した場合のp値の方が極端に小さくなっているのがわかる。

観測データに基づいて群分けをすると、真の群分けから逸脱するが、その逸脱の影響が群間の差を大きくしていると理解できる。

クラスタリングと、それに引き続く群間比較におけるSelective inferenceの考え方は、「群分けに使われた変数を、群別に比較すると、群間差が過大評価されるので、その分を調整しなければ、群間差があるという主張は言い過ぎになる。その結果、研究の再現性が下がる」と言うものである。

```{r}
my.data.original <- data.frame(cl=AB.label,X.AB)
t.test(X1 ~ cl, data=my.data.original)
```

```{r}
my.data.kmeans <- data.frame(cl=km2$cluster,X.AB)
t.test(X1 ~ cl, data=my.data.kmeans)
```



## データを用いて判断するとは

セクションイントロ

### 検定と推定と学習

### 検定

### 推定

### 学習

### 例

## 確率事象と確率モデル

セクションイントロ

### 一般論

### 具体例

カイ二乗検定もモデルで説明

尤度比検定～カイ二乗検定

## 確率と尤度

## モデル

### パラメトリックとノンパラメトリック

### パラメトリックと言われるアプローチ

### ノンパラメトリックと言われるアプローチ

## 過剰適合とバイアス=バリアンスのトレードオフ

## 線形代数と線形モデル

## モデルとアルゴリズム

### モデルと尤度関数・コスト関数


マッピングのコスト

モデルに従って厳密な尤度関数。

単なる「指標」としてのコスト関数。

コスト関数が尤度関数になっている場合(Sum of square と正規誤差に基づく対数尤度関数)。
### ペナルティ

### 最適化


