{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias\n",
    "\n",
    "機械学習では、サンプルから実際の法則を予想する。バイアスとは、予想した法則とサンプルとの誤差のことである。例えば、散布図を最小二乗法で近似すると、実際の分布と近似直線には誤差が残っている。\n",
    "\n",
    "-----\n",
    "\n",
    "# Variance\n",
    "\n",
    "機械学習に用いるサンプルを別のサンプルに取り換えると、予想される法則も変化する。この変化の度合いをバリアンスと呼ぶ。例えば、同じ実験を２回繰り返して２つの散布図を得た場合、最小二乗法でそれぞれに引いた近似直線は、傾きや切片が異なっている。\n",
    "\n",
    "-----\n",
    "\n",
    "# Bias-variance tradeoff\n",
    "\n",
    "バイアスとバリアンスには関係があり、一般に両方の値を小さくしていくことはできない。このことをバイアスとバリアンスのトレードオフと呼ぶ。このことは、以下の例から観察することができる。\n",
    "\n",
    "$xy$平面上の$10$個のサンプルから、$2$次関数$y=f(x)$のグラフを再現することを考える。点たちは誤差を含むサンプルであるため、どの程度正確に取得できているか不明である点に注意する。\n",
    "\n",
    "- バイアスを小さくしようとした場合\n",
    "\n",
    "$9$次関数を用いる方針が考えられる。こうして予想した関数$g(x)$はサンプルを完全に再現するが、実際の関数$f(x)$とは大きく異なるため、別のサンプルに対して同じ手法で予想した関数$h(x)$とは大きく異なってしまう。つまり、バリアンスが非常に大きくなってしまう。\n",
    "\n",
    "- バリアンスを小さくしようとした場合\n",
    "\n",
    "何か良いアルゴリズムを適用した結果、バリアンスが非常に小さくなったと仮定する。つまり、何かの根拠で$f(x)$が$2$次関数であることが予想され、近似$g(x)$を得られるようになったと仮定する。この時、良い近似を得たが故に、サンプルたちが持っている誤差は反映されず、近似$g(x)$はサンプルの点をほとんど通ることができない。特に、外れ値と$g(x)$は非常に遠くなる。つまり、バイアスが大きくなってしまう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
