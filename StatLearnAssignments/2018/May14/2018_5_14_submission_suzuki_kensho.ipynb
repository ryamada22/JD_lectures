{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重回帰分析のモデルとその最小二乗法の式は\n",
    "\n",
    "$$ \\hat{y} = \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_m x_m + \\beta_{m+1} \\\\ {\\parallel y - X \\hat{\\beta} \\parallel^2 } \\Leftrightarrow {\\parallel y - \\hat{y}\\parallel^2 }$$\n",
    "\n",
    "と表される。\n",
    "\n",
    "\n",
    "### 1. stepwise regression\n",
    "主に次の二つの方法がある。\n",
    "\n",
    "- forward stepwise : 説明変数の数を0から始めて、一つずつ増やしていく方法。\n",
    "- backward stepwise : 全ての説明変数から始めて、一つずつ減らしていく方法。\n",
    "\n",
    "手順としては以下の通り。\n",
    "① 回帰に使用されていない(されている)変数を一つ追加(除外)して、後に述べる評価指標の値を計算。\n",
    "② 計算し終わったら、追加(除外)した説明変数を元に戻す。\n",
    "③ 追加(除外)する変数を別のものに変えて、評価指標の値を計算。\n",
    "④ 全ての使用されていない(されている)変数について評価指標を計算し、その値が最も小さくなった変数を実際に追加(除外)する。\n",
    "⑤ ①〜④を繰り返し行う。\n",
    "\n",
    "評価指標について、<br>\n",
    "Mallows's Cp\n",
    "$$ C_p = \\frac{SSE}{S^2} - n + 2m $$\n",
    "AIC(赤池情報量基準)\n",
    "$$ AIC = m\\log (\\frac{SSE}{m}) + 2 $$\n",
    "BIC(Bayesian Information Criterion)\n",
    "$$ BIC = m\\log (\\frac{SSE}{m}) + n\\log (m) \\\\ n : サンプル数 \\\\ m : 回帰モデルを構築した説明変数の数 \\\\ S^2 : すべての説明変数を使用して回帰分析したときの誤差の二乗(最小二乗法)の平均 \\\\ SSE : 構築されている回帰モデルによる推測値と真値の誤差の二乗和$$\n",
    "\n",
    "などがある。\n",
    "\n",
    "\n",
    "stepwise回帰によって、上記のように変数選択を行うことができる。\n",
    "\n",
    "\n",
    "### 2. LASSO regression\n",
    "\n",
    "$$ S_{\\lambda}(\\beta) = {\\parallel y - X \\hat{\\beta} \\parallel^2 + \\lambda {\\parallel \\hat{\\beta} \\parallel} } $$\n",
    "\n",
    "LASSO回帰は上記の式において、$S_{\\lambda}(\\beta)$が最小となるような$\\beta$を探索する回帰法である。\n",
    "$\\beta$の値に制約がかかるため、その絶対値の大きさは$\\lambda$が大きいとき、stepwise回帰の時よりも小さくなる。(shrinkage)\n",
    "\n",
    "また、$\\lambda$が大きくなるにつれ、回帰に使用される説明変数が減っていく。(subset selection)\n",
    "\n",
    "$\\beta$の求め方の一つに、CD(Coordinate Descent)という方法がある。\n",
    "CDでは、各パラメータ毎に誤差関数を微分して更新式を得て、それを用いて更新を繰り返し行うことにより収束した最適な推定値を得ることができる。\n",
    "\n",
    "欠点として、\n",
    "- 変数選択の一致性が保証されていない。\n",
    "- 説明変数の数が訓練データの数より大きいとき、多くても訓練データの数程度の説明変数を用いた回帰しかできない。 \n",
    "- 相関の高い説明変数群がある場合、その中の一つのみを変数として選択することが多い。\n",
    "が挙げられる。\n",
    "\n",
    "\n",
    "### 3. Ridge regression\n",
    "\n",
    "$$ S_{\\lambda}(\\beta) = {\\parallel y - X \\hat{\\beta} \\parallel^2 + \\lambda {\\parallel \\hat{\\beta} \\parallel}^2 } $$\n",
    "\n",
    "Ridge回帰は上記の式において、$S_{\\lambda}(\\beta)$が最小となるような$\\beta$を探索する回帰法である。\n",
    "$\\beta$の値にLASSO回帰よりも強い制約がかかるため、その絶対値の大きさは$\\lambda$が大きいとき、stepwise回帰やLASSO回帰の時よりも小さくなる。(shrinkage)\n",
    "また、制約は超球体によるものであり、変数の削減は起こりにくい。\n",
    "\n",
    "上式は$\\beta$で偏微分可能なので、その値が0となるような$\\beta$が最適解となる。\n",
    "\n",
    "\n",
    "### 4. Elastic Net\n",
    "\n",
    "$$ S_{\\lambda}(\\beta) = {\\parallel y - X \\hat{\\beta} \\parallel^2 + \\alpha \\lambda {\\parallel \\hat{\\beta} \\parallel}　+ (1 - \\alpha) \\lambda {\\parallel \\hat{\\beta} \\parallel}^2 } \\\\ \\alpha (0 < \\alpha < 1)$$\n",
    "\n",
    "LASSO回帰とRidge回帰を組み合わせた方法で、LASSOの弱点を補うために、割合でRidge回帰の項を含んでいる。\n",
    "$\\alpha = 0$の場合はRidge回帰、$\\alpha = 1$の場合はLASSO回帰に帰着する。\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "参照 : http://tekenuko.hatenablog.com/entry/2017/11/18/214317\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
