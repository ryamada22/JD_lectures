{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there any improvement in your skills to grab the content of the review? If any, describe what they are and how they worked?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review divided steps to do machine learning into few steps: preparation, optimization and accuracy, and consultation.  \n",
    "  \n",
    "**Preparation**  \n",
    "Similar to my last encounter with machine learning for beginner few years ago, the review put emphasis on having a clean data before starting to splice it for tranining, test, validaition, and work algorithms on top of it. However, while in the class, Prof. Yamada slightly emphasis that removing outliers data is not okay, i wonder if its okay to remove outlier if its indicated as badly recorded data (or having the prior knowledge that the data is broken or falsely recorded) as the article suggest, or round the data to the upper limit instead. The article also suggest a normalization, either to a certain min-max value or to a certain range with certain mean and standard deviation.  \n",
    "After being assured that the data is clean, we can proceed to split the data into training, validation, and test; pick the algorithm category (whether it is classification problem, regression problem, or unsupervised problem); pick the algorithm; and take care the imbalanced data problem.  \n",
    "In my understanding, this preparation step emphasis the GIGO principle (Garbage In Garbage Out) which is as beginners in machine learning is rather easy to forget. The writer uses **half** of the article tips the readers to have a clean data set and the other half is about optimization, accuracy, and the importance of consultation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "**Optimization and Accuracy**  \n",
    "The optimization steps in the article discuss about choosing the right hyper-parameter (e.g. the _k_ in _k_-NN or _k_-means) --based on the accuracy or MCC score, using grid search or other automatic toolkit such as auto-sklearn-- and battle the over-fitting through CV and regularization. This process could be time-consuming and process-heavy especially if we deal with a gigantic data sets. The article suggest we take a tiny part of the data to practice before employ the process to the entire dataset to save time and effort; and shuffle the dataset to avoid any possible trend related to the order the data instances were collected, and which might wrongly influence the learning process.  \n",
    "In  class we learned that for regularization we can employ Ridge or Lasso depending on our requirement of interpreting the result.  \n",
    "Finally, after training the actual model, we can measure the accuracy. The article gave clue about the risk of having an accuracy measured by just an accuracy relying on the ratio of true positives and true negatives versus the total number of data in a binary classification task. Instead they mentioned that we can use additional measurement to ensure that the data doesn't behave accurate because it is imbalanced or even behave similarly as random guessing (having close to 0 in MCC score).  \n",
    "In case not having binary labels for classification result, we can use ROC or Precision-Recall curve and measure the area under the curve as a metrics for accuracy. Other notable mention for checking the model performance is permutation testing and bootstrapping.  \n",
    "The focus on the accuracy dicussion from the article is that having a **high** score accuracy from validation or test data is sometimes not enough. The user have to be certain that the accuracy result is calculated impartial to the data behaviour (or misbehaviour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "**Consultation**\n",
    "The article informs the importance on using open-sourced software and code base to ease debug problem or consultation to expert and/or online forums for problems arise during a machine learning project. Since the community of machine learning is pretty much active and responsive, the community may notice some mistake or inefficiency in codes that we initially don't. Using a open-sourced software and code base increases the possibility of these and also collaboration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What i learn from the class\n",
    "While i already have the basics of machine learning concept, i learned from the class on how to interpret machine learning concepts concisely and how regularization/optimization work. Notably on why certain algorithm is best for certain task or why certain calculation behave the way it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement to the lecture\n",
    "It is hard to convey the basic essence of machine learning to students with various knowledge background in 1.5 hours per week without homework and additional effort from the student itself. I especially like how Prof. Yamada conduct a _forced_ discussion among students by throwing a snowball to draw conclusion or understanding on a certain topic or terminology since it forces students to spew their current knowledge to the particular question and Prof. Yamada can drive the student responses toward a common definition.  \n",
    "My general complaint is just the class location. While in the beginning, the class shouldn't be where the class was held at, a more common location without restricted access is probably preferrable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
