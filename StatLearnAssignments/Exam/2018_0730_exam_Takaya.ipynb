{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StatGenet class Machine Learning exam\n",
    "\n",
    "**Read \"Ten quick tips for machine lkearning in computational biology\".**\n",
    "\n",
    "**Is there any improvement in your skills to grab the content of the review?**\n",
    "\n",
    "**If any, describe what they are and how they worked?**\n",
    "\n",
    "**If no,  describe how the lectures should be improved.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into training dataset and test dataset.\n",
    "\n",
    "When we would like to estimate some kinds of values or classify into some groups from given dataset, we can use machine learning.  Suppose that an algorithm of machine learning can be determined, we should evaluate the algorithm and adjust it to make estimation or classification better. Therefore, for separating the given dataset into training dataset and test dataset, we evaluate and adjust the algorithm in training dataset, then we apply the adjusted algorithm to test dataset as unseen data.   \n",
    "<br>\n",
    "<br>\n",
    "  \n",
    "#### Clarifying what we would like to know.\n",
    "\n",
    "We have learnt the method of machine learning in class, however, we should clarify what should be output with machine learning; e.g. value, formula, label. If we want to know the real values from a dataset, we should use regression methods (least square, polynomial, Ridge, LASSO, etc.). If we want to know the class of unseen data, we should use classification methods like k-nearest neighbor or support vector machine.\n",
    "<br>\n",
    "<br>\n",
    "  \n",
    "#### Bias and Variance and Overfitting.\n",
    "\n",
    "Bias is an error from the truth. When we apply a learning algorithm on a sample dataset, its output data distributes with somehow different from the truth. Bias is the difference between the truth and the output distribution. Variance is the range within the output distribution. So, outputs from the low bias model are close to the truth, and outputs from the low variance model are close to each other. However, bias and variance have the relationship of trade-off, that means a lower bias model has high variance and vice versa. The more number of parameters of a model (the more the model complexity), the higher the variance, and this situation is called overfitting. Overfitting is the excessive fitness of parameters to the training dataset. The accuracy of the training dataset increase for tuning parameters of the algorithm for the training dataset, but this algorithm is not adaptable for the unseen dataset (test dataset). Regularization is a way for avoiding overfitting to penalize against increasing complexity of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
