{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evaluation of Learning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## What is the best method?\n",
    "\n",
    "Assume a population with a distribution.\n",
    "\n",
    "The best method is one that re-constructs the distribution from a sample set.\n",
    "\n",
    "Because one sample set is different from another sample set, the best method should be able to re-construct the population distribution from different sample sets most \"appropriately\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bias and Variance\n",
    "\n",
    "+ The learning method does not aim at the TRUTH and the the deviation of the aim of method and the truth is called bias.\n",
    "\n",
    "+ The learning method returns various results depending on the sample sets. The variation around the \"goal\" of method is called variance.\n",
    "\n",
    "Read [Bias and Variance](../cells/Bias_and_Variance.ipynb).\n",
    "\n",
    "[バイアスとバリアンス](../cells/Bias_and_Variance.ipynb)を読め。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "The more model is complicated, the fewer is the errors of training set.\n",
    "\n",
    "When the model is optimally complicated, the errors of test set become fewest.\n",
    "\n",
    "When the model is both less and more complicated than the optimally complicated, the errors of test set become more.\n",
    "\n",
    "When the model is simple, bias is big but variance is small.\n",
    "\n",
    "When the model is complicated, bias is small but variance is big.\n",
    "\n",
    "To make the errors of test set fewest, the model should be optimally complicated, meaning bias and variance should be not too big or not too small and balance between bias and variance should be adjusted.\n",
    "\n",
    "Read [Bias-Variance Tradeoff](../cells/Bias-Variance_Tradeoff.ipynb).\n",
    "\n",
    "[バイアス-バリアンス　トレードオフ](../cells/Bias-Variance_Tradeoff.ipynb)を読め。\n",
    "\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "The error rate of training samples is lower than one of test samples.\n",
    "\n",
    "In other words, the classification rule performs too good for the training samples, compared with the test samples. This \"too-good\" performance is called \"overfitting\".\n",
    "\n",
    "Generate a new sample set (test set) and check how many new samples are labeled wrong (error rate of test samples).\n",
    "\n",
    "See the error rate of test samples below is higher than the error rate of training samples above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
